import { useEffect, useRef, useCallback, useState } from 'react';
import { useVoiceStore, useVoiceSettings } from '@/stores/useVoiceStore';
import { useUserSettings } from '@/hooks/useUserSettings';
import { voiceOrchestrator } from '@/services/VoiceOrchestrator';
import { createNariLabsProvider } from '@/services/tts/NariLabsProvider';
import { createBrowserTTSProvider } from '@/services/tts/BrowserTTSProvider';
import { webRTCService } from '@/services/voice/WebRTCConnectionService';
import { vadService } from '@/services/voice/VADService';
import type { UseChat } from 'ai/react';

/**
 * Main voice hook that orchestrates the entire voice system
 * 
 * Implements settings-dependent initialization to fix race conditions
 * Manages connection lifecycle and integrates with Vercel AI SDK
 */
export const useVoice = (chat?: UseChat) => {
  const voiceStore = useVoiceStore();
  const { settings: voiceSettings, settingsLoaded: voiceSettingsLoaded, updateSettings: updateVoiceSettings } = useVoiceSettings();
  const { settings: userSettings, loading: userSettingsLoading } = useUserSettings();
  
  // Refs for persistent objects (prevents re-initialization on re-renders)
  const connectionRef = useRef<typeof webRTCService | null>(null);
  const vadRef = useRef<typeof vadService | null>(null);
  const initializationPromise = useRef<Promise<void> | null>(null);
  
  // Local state for initialization tracking
  const [initializationStatus, setInitializationStatus] = useState<
    'idle' | 'initializing' | 'initialized' | 'failed'
  >('idle');

  // Settings synchronization effect
  useEffect(() => {
    if (!userSettingsLoading && userSettings?.pam_preferences && !voiceSettingsLoaded) {
      console.log('âš™ï¸ Synchronizing voice settings from user preferences');
      
      const pamPrefs = userSettings.pam_preferences;
      updateVoiceSettings({
        enabled: pamPrefs.voice_enabled ?? true, // Default to enabled as fallback
        voice: pamPrefs.voice_preference || 'alloy',
        rate: pamPrefs.speech_rate || 1.0,
        volume: pamPrefs.volume || 0.8,
        autoPlay: pamPrefs.auto_play ?? true,
      });
      
      voiceStore.setSettingsLoaded(true);
    }
  }, [userSettings, userSettingsLoading, voiceSettingsLoaded, updateVoiceSettings, voiceStore]);

  // Main initialization effect - waits for settings to be loaded
  useEffect(() => {
    const initializeVoiceSystem = async (): Promise<void> => {
      try {
        console.log('ðŸŽ™ï¸ Starting voice system initialization...');
        setInitializationStatus('initializing');
        voiceStore.setAgentStatus('connecting');

        // Step 1: Initialize TTS providers with fallback chain
        const providers = [
          createNariLabsProvider(), // Primary
          createBrowserTTSProvider(), // Fallback
        ];
        
        await voiceOrchestrator.initialize(providers);
        console.log('âœ… Voice orchestrator initialized');

        // Step 2: Initialize WebRTC connection (if enabled in settings)
        if (settings.enableWebRTC !== false) {
          try {
            const signalingUrl = import.meta.env.VITE_WEBRTC_SIGNALING_URL || 
                               'wss://wheels-wins-backend-staging.onrender.com/api/v1/webrtc';
            
            await webRTCService.connect({
              signalingUrl,
              enableDataChannel: true,
              enableAudioStream: true,
            });
            
            connectionRef.current = webRTCService;
            console.log('âœ… WebRTC connection initialized');
          } catch (error) {
            console.warn('âš ï¸ WebRTC initialization failed, continuing with WebSocket fallback:', error);
          }
        }
        
        // Step 3: Initialize VAD service  
        try {
          await vadService.initialize({
            algorithm: 'webrtc',
            speechThreshold: settings.vadThreshold || 0.6,
            silenceDuration: settings.endpointingSilenceDuration || 800,
            enableWakeWord: false, // Can be enabled later
            onSpeechStart: () => {
              console.log('ðŸ—£ï¸ User started speaking');
              // Handle speech start (e.g., pause TTS)
              if (voiceStore.agentStatus === 'speaking') {
                voiceOrchestrator.interrupt();
              }
            },
            onSpeechEnd: () => {
              console.log('ðŸ”‡ User stopped speaking');
              // Handle speech end (e.g., process user input)
            },
            onVolumeChange: (volume) => {
              // Update volume visualization if needed
            }
          });
          
          vadRef.current = vadService;
          console.log('âœ… VAD service initialized');
        } catch (error) {
          console.warn('âš ï¸ VAD initialization failed:', error);
        }

        // Step 4: Mark as initialized
        voiceStore.setConnectionId(`voice_${Date.now()}`);
        voiceStore.setAgentStatus('connected');
        setInitializationStatus('initialized');
        
        console.log('âœ… Voice system initialization completed');

      } catch (error) {
        console.error('âŒ Voice system initialization failed:', error);
        voiceStore.setError(`Voice initialization failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
        voiceStore.setLastError(error instanceof Error ? error : new Error(String(error)));
        setInitializationStatus('failed');
      }
    };

    // Only initialize when:
    // 1. Settings are loaded
    // 2. Voice is enabled
    // 3. Not already initialized or initializing
    const shouldInitialize = 
      voiceSettingsLoaded && 
      voiceSettings.enabled && 
      voiceStore.isVoiceEnabled &&
      initializationStatus === 'idle' &&
      !initializationPromise.current;

    if (shouldInitialize) {
      console.log('ðŸš€ Voice initialization conditions met, starting initialization...');
      initializationPromise.current = initializeVoiceSystem();
    }

    // Cleanup when voice is disabled
    if (voiceSettingsLoaded && (!voiceSettings.enabled || !voiceStore.isVoiceEnabled)) {
      if (initializationStatus === 'initialized') {
        console.log('ðŸ”‡ Voice disabled, cleaning up...');
        cleanup();
        setInitializationStatus('idle');
        initializationPromise.current = null;
      }
    }

  }, [
    voiceSettingsLoaded, 
    voiceSettings.enabled, 
    voiceStore.isVoiceEnabled, 
    initializationStatus,
    voiceStore
  ]);

  // Cleanup function
  const cleanup = useCallback(() => {
    console.log('ðŸ§¹ Cleaning up voice system...');
    
    // Cancel any ongoing speech
    voiceOrchestrator.cancelSpeech();
    
    // Clean up WebRTC connection
    if (connectionRef.current === webRTCService) {
      webRTCService.disconnect();
      connectionRef.current = null;
    }
    
    // Clean up VAD service
    if (vadRef.current === vadService) {
      vadService.destroy();
      vadRef.current = null;
    }
    
    // Reset store state
    voiceStore.reset();
    
  }, [voiceStore]);

  // Cleanup effect
  useEffect(() => {
    return cleanup;
  }, [cleanup]);

  // Main speak function - integrates with Vercel AI SDK
  const speak = useCallback(async (
    text: string, 
    options: {
      priority?: 'low' | 'normal' | 'high' | 'urgent';
      chunkId?: string;
      fallbackToText?: boolean;
    } = {}
  ) => {
    if (initializationStatus !== 'initialized') {
      console.warn('âš ï¸ Voice system not initialized, cannot speak');
      return;
    }

    try {
      await voiceOrchestrator.speak(text, {
        ...options,
        fallbackToText: options.fallbackToText ?? true,
      });
    } catch (error) {
      console.error('âŒ Speak failed:', error);
      voiceStore.setError(`Speech failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }, [initializationStatus, voiceStore]);

  // Start voice recording
  const startListening = useCallback(() => {
    if (vadRef.current) {
      vadRef.current.start();
      voiceStore.setAgentStatus('listening');
    } else {
      console.warn('âš ï¸ VAD not initialized');
    }
  }, [voiceStore]);

  // Stop voice recording
  const stopListening = useCallback(() => {
    if (vadRef.current) {
      vadRef.current.stop();
      voiceStore.setAgentStatus('connected');
    }
  }, [voiceStore]);

  // Interrupt function for barge-in
  const interrupt = useCallback(() => {
    console.log('âš ï¸ Voice interrupt requested');
    voiceOrchestrator.interrupt();
    
    // Stop any ongoing playback
    voiceStore.clearAudioQueue();
    
    // Resume listening if VAD is available
    if (vadRef.current) {
      vadRef.current.resume();
    }
  }, [voiceStore]);

  // Mute/unmute functions
  const mute = useCallback(() => {
    voiceStore.toggleMute();
  }, [voiceStore]);

  const unmute = useCallback(() => {
    if (voiceStore.isMuted) {
      voiceStore.toggleMute();
    }
  }, [voiceStore]);

  // Connection status checker
  const isConnected = voiceStore.agentStatus === 'connected' || 
                     voiceStore.agentStatus === 'listening' || 
                     voiceStore.agentStatus === 'speaking';

  // Integration with Vercel AI SDK chat
  useEffect(() => {
    if (!chat || initializationStatus !== 'initialized') return;

    // Listen for new assistant messages from the chat
    const handleNewMessage = (message: any) => {
      if (message.role === 'assistant' && message.content && voiceSettings.autoPlay) {
        console.log('ðŸ¤– New assistant message, speaking response');
        speak(message.content, { priority: 'normal' });
      }
    };

    // This would need to be implemented based on the specific chat hook used
    // For now, it's a placeholder for the integration point
    
  }, [chat, initializationStatus, speak, voiceSettings.autoPlay]);

  // Return hook interface
  return {
    // Status
    status: voiceStore.agentStatus,
    isConnected,
    isInitialized: initializationStatus === 'initialized',
    isInitializing: initializationStatus === 'initializing',
    error: voiceStore.error,
    
    // Voice activity
    isListening: voiceStore.agentStatus === 'listening',
    isSpeaking: voiceStore.agentStatus === 'speaking',
    isUserSpeaking: voiceStore.isUserSpeaking,
    
    // Settings
    settings: voiceSettings,
    settingsLoaded: voiceSettingsLoaded,
    updateSettings: updateVoiceSettings,
    
    // Controls
    speak,
    startListening,
    stopListening,
    interrupt,
    mute,
    unmute,
    isMuted: voiceStore.isMuted,
    
    // Queue info
    queueSize: voiceStore.audioQueue.length,
    isPlaying: voiceStore.activePlaybackId !== null,
    
    // Connection info
    connectionId: voiceStore.connectionId,
    metrics: voiceStore.metrics,
    
    // Advanced controls
    cancel: voiceOrchestrator.cancelSpeech.bind(voiceOrchestrator),
    reset: cleanup,
    
    // VAD status
    vadStatus: vadRef.current?.getStatus() || null,
    
    // WebRTC status
    webRTCStatus: connectionRef.current?.getStatus() || null,
  };
};

