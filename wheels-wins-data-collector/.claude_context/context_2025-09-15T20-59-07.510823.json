{
  "timestamp": "2025-09-15T20:59:07.511122",
  "project_root": "/Users/thabonel/Code/wheels-wins-landing-page/wheels-wins-data-collector",
  "recent_changes": [
    "wheels-wins-data-collector/.claude_context/context_2025-09-15T20-28-16.070564.json",
    "wheels-wins-data-collector/.claude_context/memory_keeper_summary.json",
    "wheels-wins-data-collector/PROJECT_CONTEXT.md"
  ],
  "last_commit": "chore: automated context update",
  "git_status": "M backend/app/services/trip_scraper.py\n M scraper_service/main.py\n M wheels-wins-data-collector/scrapers/real_attractions_scraper.py\n M wheels-wins-data-collector/scrapers/real_camping_scraper.py\n M wheels-wins-data-collector/scrapers/real_parks_scraper.py\n?? docs/sql-fixes/15_create_trip_scraper_tables.sql\n?? wheels-wins-data-collector/.claude_context/context.db\n?? wheels-wins-data-collector/requirements-context.txt\n?? wheels-wins-data-collector/scripts/\n?? wheels-wins-data-collector/test_google_places.py",
  "key_files": {
    "README.md": "# Wheels & Wins Autonomous Data Collector\n\n## \ud83d\ude80 Production Data Collection System\n\nAutonomous data collection service that runs weekly on Render to build and maintain a comprehensive database of 5000+ travel locations. This system complements the admin-controlled scraper in the main application.\n\n## Architecture Overview\n\n### Dual-System Strategy\n1. **Data Collector** (this system)",
    ".pytest_cache/README.md": "# pytest cache directory #\n\nThis directory contains data from the pytest's cache plugin,\nwhich provides the `--lf` and `--ff` options, as well as the `cache` fixture.\n\n**Do not** commit this to version control.\n\nSee [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.\n",
    "PROJECT_CONTEXT.md": "# Wheels & Wins Data Collector - Project Context\n\n*Last Updated: 2025-09-15*\n\n## Project Overview\n\nThe Wheels & Wins data collector is a critical backend service that autonomously scrapes and maintains location data for camping sites, parks, and attractions. This service feeds the main Wheels & Wins trip planning application with high-quality, up-to-date location information.\n\n### Architecture\n- **Language**: Python with asyncio for concurrent operations",
    "main_autonomous.py": "#!/usr/bin/env python3\n\"\"\"\nAutonomous Data Collection System for Render Cron Job\nCollects thousands of real travel locations monthly\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path",
    "insert_test_data.py": "#!/usr/bin/env python3\n\"\"\"\nInsert test data directly to Supabase to verify tables are working\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv\nfrom datetime import datetime",
    "load_swimming_spots_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive swimming spots and waterfalls data to trip_templates\nThis includes beaches, rivers, lakes, hot springs, and waterfalls\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "load_usa_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive USA travel data to trip_templates\nThis includes real locations with accurate coordinates\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "check_performance.py": "#!/usr/bin/env python3\n\"\"\"\nPerformance Check Script for Wheels & Wins Data Collector\nChecks if the scraper is meeting the 500 trips/week target\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nfrom datetime import datetime, timedelta",
    "test_with_sample_data.py": "#!/usr/bin/env python3\n\"\"\"\nTest the system with sample data to verify end-to-end functionality\n\"\"\"\n\nimport asyncio\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n",
    "main_autonomous 2.py": "#!/usr/bin/env python3\n\"\"\"\nAutonomous Data Collection System for Render Cron Job\nCollects thousands of real travel locations monthly\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path",
    "test_collection.py": "#!/usr/bin/env python3\n\"\"\"\nTest Script for Wheels & Wins Data Collection System\n\nRun a small test collection to verify everything works before full collection.\n\nUsage:\n    python test_collection.py\n\"\"\"\n",
    "load_comprehensive_australia_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive Australia travel data to trip_templates\nThis includes real locations with accurate coordinates\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "setup_check.py": "#!/usr/bin/env python3\n\"\"\"\nSetup Check Script - Verify the data collection system is ready to run\n\nThis script checks:\n1. Python dependencies\n2. Environment variables\n3. Directory structure\n4. API connectivity\n5. Database connectivity",
    "test_autonomous.py": "#!/usr/bin/env python3\n\"\"\"\nTest the autonomous collector locally before Render deployment\n\"\"\"\n\nimport asyncio\nimport os\nimport sys\nfrom pathlib import Path\nfrom dotenv import load_dotenv",
    "load_new_zealand_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive New Zealand travel data to trip_templates\nThis includes real locations with accurate coordinates\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "test_google_places.py": "#!/usr/bin/env python3\n\"\"\"\nTest Google Places API Integration\nQuick test to verify Google Places API is working correctly\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path",
    "simple_test.py": "#!/usr/bin/env python3\n\"\"\"\nSimple validation test - just check if the system can start and connect to Supabase\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n",
    "load_canada_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive Canada travel data to trip_templates\nThis includes real locations with accurate coordinates\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "load_camping_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive camping data (free camping & RV parks) to trip_templates\nThis includes boondocking, wild camping, and full-service RV resorts\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "load_great_britain_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive Great Britain travel data to trip_templates\nThis includes real locations with accurate coordinates\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "test_collector.py": "#!/usr/bin/env python3\n\"\"\"\nTest script for the data collector\nRun this locally to verify everything works before deploying\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path",
    "main.py": "#!/usr/bin/env python3\n\"\"\"\nWheels & Wins Mass Data Collection System\nMain orchestrator for collecting travel data across 5 countries\n\n\u26a0\ufe0f STANDALONE SYSTEM - SAFE TO DELETE AFTER USE \u26a0\ufe0f\n\nThis script coordinates the massive one-time data collection for:\n- Australia, New Zealand, Canada, US, Great Britain\n- National parks, camping spots, attractions, swimming spots, waterfalls",
    "create_tables_directly.py": "#!/usr/bin/env python3\n\"\"\"\nCreate missing tables directly in Supabase\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv\n",
    "main_daily.py": "#!/usr/bin/env python3\n\"\"\"\nDaily Data Collection System for Render Cron Job\nCollects ~72 travel locations daily to reach 500/week target\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path",
    "load_travel_data_to_templates.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive travel data directly to trip_templates table\nThis works with the existing database structure\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "load_hiking_trails_data.py": "#!/usr/bin/env python3\n\"\"\"\nLoad comprehensive hiking trails and outdoor activities data to trip_templates\nThis includes day hikes, multi-day treks, lookouts, and outdoor adventures\n\"\"\"\n\nimport asyncio\nfrom supabase import create_client, Client\nimport os\nfrom dotenv import load_dotenv",
    "data_processors/validator.py": "\"\"\"\nData Validator\n\nValidates collected location data for quality and completeness:\n- Required field validation\n- Coordinate validation\n- Data format validation\n- Business logic validation\n- Accessibility and safety checks\n\"\"\"",
    "data_processors/deduplicator.py": "\"\"\"\nData Deduplicator\n\nRemoves duplicate locations using multiple strategies:\n- Geographic proximity clustering\n- Name similarity matching\n- Source reliability prioritization\n- Coordinate precision analysis\n\"\"\"\n",
    "data_processors/__init__.py": "# Data processors module for Wheels & Wins Data Collection System",
    "data_processors/enhancer.py": "\"\"\"\nData Enhancer\n\nEnhances location data using AI and additional sources:\n- Generates missing descriptions using AI\n- Enriches with additional metadata\n- Standardizes data formats\n- Adds semantic tags and categories\n- Improves search optimization\n\"\"\"",
    "scripts/intelligent-context-saver.py": "#!/usr/bin/env python3\n\"\"\"\nIntelligent Context Saver for Claude Code Sessions\nAutomatically saves project context at meaningful development milestones\n\"\"\"\n\nimport os\nimport json\nimport time\nimport hashlib",
    "scripts/test-intelligence.py": "#!/usr/bin/env python3\n\"\"\"\nTest the Intelligent Context Saver\nDemonstrates the intelligence criteria and thresholds\n\"\"\"\n\nimport asyncio\nimport sys\nfrom pathlib import Path\n",
    "scripts/unified-context-server.py": "#!/usr/bin/env python3\n\"\"\"\nUnified Context Continuity MCP Server\nCombines token-aware monitoring with file-change intelligence\nProvides seamless context restoration across Claude Code conversations\n\"\"\"\n\nimport asyncio\nimport json\nimport logging",
    "scripts/test-unified-system.py": "#!/usr/bin/env python3\n\"\"\"\nTest the Unified Context Continuity System\nDemonstrates the seamless integration of token-aware + file-intelligence\n\"\"\"\n\nimport asyncio\nimport sys\nimport json\nfrom pathlib import Path",
    "scrapers/swimming_spots.py": "\"\"\"\nSwimming Spots Scraper Service\n\nCollects swimming locations including beaches, lakes, pools, rivers, and waterfalls\nacross Australia, New Zealand, Canada, US, and Great Britain.\n\nData Sources:\n- Government beach databases\n- Tourism board beach/lake listings\n- Swimming hole databases",
    "scrapers/__init__.py": "# Scrapers module for Wheels & Wins Data Collection System",
    "scrapers/camping_spots.py": "\"\"\"\nCamping Spots Scraper Service\n\nCollects comprehensive camping data including free camping, RV parks, and campgrounds\nacross Australia, New Zealand, Canada, US, and Great Britain.\n\nData Sources:\n- WikiCamps (Australia/NZ)\n- iOverlander (Global)\n- Campendium (US/Canada)",
    "scrapers/real_camping_scraper.py": "\"\"\"\nReal Camping Spots Scraper with Actual API Integrations\nThis replaces the placeholder implementation with real data sources\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport logging\nfrom typing import List, Dict, Optional, Any\nfrom datetime import datetime",
    "scrapers/attractions.py": "\"\"\"\nTourist Attractions Scraper Service\n\nCollects comprehensive tourist attraction data including landmarks, museums, \nscenic viewpoints, and other points of interest across 5 countries.\n\nData Sources:\n- Tourism board APIs\n- Google Places API\n- TripAdvisor data",
    "scrapers/real_parks_scraper.py": "\"\"\"\nReal Parks Scraper with Actual API Integrations\nCollects data from National Park Services, OpenStreetMap, and other sources\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport logging\nfrom typing import List, Dict, Optional, Any\nfrom datetime import datetime",
    "scrapers/national_parks.py": "\"\"\"\nNational Parks Scraper Service\n\nCollects comprehensive national park data from government APIs and official sources\nacross Australia, New Zealand, Canada, US, and Great Britain.\n\nData Sources:\n- Australia: Parks Australia, state park services\n- New Zealand: Department of Conservation (DOC)\n- Canada: Parks Canada, provincial parks",
    "services/database_state.py": "\"\"\"\nDatabase State Management for Data Collector\nReplaces file-based progress tracking with robust Supabase integration\n\"\"\"\n\nimport json\nimport logging\nimport hashlib\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timedelta",
    "services/photo_scraper.py": "\"\"\"\nLocation Photo Scraper Service\nAdapts the frontend photo scraping system for individual scraped locations\nUses Wikipedia, Google Search, and Tourism APIs to find representative photos\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport logging\nimport hashlib",
    "services/monitoring.py": "\"\"\"\nMonitoring and Alerting Service for Data Collector\nSends notifications and tracks health metrics\n\"\"\"\n\nimport os\nimport json\nimport logging\nimport aiohttp\nfrom typing import Dict, List, Optional",
    "services/photo_storage.py": "\"\"\"\nPhoto Storage Service for Supabase\nHandles downloading and storing location photos in Supabase Storage\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport logging\nimport hashlib\nfrom typing import Optional, Dict"
  },
  "current_issues": [],
  "recent_fixes": [
    "43cb2892 fix: resolve critical Decimal serialization errors in data collector",
    "6a273032 fix: resolve coordinate comparison error in swimming spot parser",
    "c5656f45 fix: resolve merge conflict in api.ts - maintain production backend URL for main branch",
    "2e3a201d fix: resolve scraper parsing errors with type safety and coordinate validation",
    "3d836b76 fix: resolve trip scraper performance and template image issues"
  ],
  "next_steps": [
    "intelligent-context-saver.py:330 - # Look for TODO comments in code",
    "intelligent-context-saver.py:335 - if 'TODO' in line or 'FIXME' in line:"
  ]
}