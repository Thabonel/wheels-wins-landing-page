# PRD-PAM-007: Comprehensive Test Coverage

**Document ID:** PRD-PAM-007
**Date:** January 29, 2026
**Type:** Testing Infrastructure
**Status:** Active
**Priority:** HIGH

---

## Executive Summary

**Current State:** 15% frontend coverage, 0% backend coverage
**Target State:** 85%+ coverage across frontend and backend
**Estimated Effort:** 30-40 hours (5-6 days)
**Business Impact:** Production confidence, faster deployments, fewer bugs

### Success Metrics

**Frontend:**
- Current: 15.2% coverage (95 tests)
- Target: **85%+ coverage** (300+ tests)

**Backend:**
- Current: 0% coverage (0 tests)
- Target: **85%+ coverage** (200+ tests)

**Testing Dashboard:**
- Current: Mock data only
- Target: **Real-time metrics** from actual test runs

---

## Problem Statement

### Current Testing Status

**Frontend Coverage: 15.2%** âš ï¸

âœ… **Covered (95 tests):**
- Auth components (AuthContext, useAuth) - 7 tests
- API utilities - 9 tests
- UI components (Button, Hero, Features, Layout) - 17 tests
- Error handling (ErrorBoundary) - 4 tests
- Calculations (trip, financial) - 31 tests
- Formatters - 10 tests
- Supabase client - 5 tests
- Other utilities - 12 tests

âŒ **Not Covered (0 tests):**
- **PAM Voice components** - Complex websocket logic
- **News module** - RSS feed aggregation
- **Social components** - Posts, comments, likes
- **Dashboard components** - Calendar, widgets
- **Trip planning components** - Route optimization
- **Budget components** - Expense tracking
- **Profile components** - User settings
- **Shop components** - Product search

**Backend Coverage: 0%** ğŸš¨

âŒ **Not Covered:**
- **88 PAM tools** - No tool tests exist
- **API endpoints** - 40+ endpoints untested
- **Database operations** - No DB tests
- **Exception handling** - Validation not tested
- **WebSocket handlers** - Real-time not tested
- **Authentication** - JWT verification untested

### Business Impact

**Without Tests:**
- ğŸš¨ **High risk deployments** - Unknown if changes break features
- ğŸ› **Bugs reach production** - No safety net
- ğŸ”¥ **Slow development** - Fear of breaking things
- â±ï¸ **Manual testing overhead** - Time-consuming
- ğŸ˜° **Stress on team** - No confidence in code

**With 85% Coverage:**
- âœ… **Confident deployments** - Know what works
- âœ… **Catch bugs early** - Before production
- âœ… **Fast development** - Safe to refactor
- âœ… **Automated validation** - CI/CD pipeline
- âœ… **Peace of mind** - Sleep well at night

---

## Scope

### In Scope

**Phase 1: Backend Testing Infrastructure** (8-10 hours)
- Create pytest test infrastructure
- Set up fixtures and mocks
- Configure coverage reporting
- Write tests for 88 PAM tools
- Test API endpoints
- Test database operations

**Phase 2: Frontend Testing Expansion** (10-12 hours)
- Add missing component tests
- Test user flows
- Test state management
- Achieve 85%+ coverage

**Phase 3: Testing Dashboard Integration** (4-6 hours)
- Connect to real test runners
- Display actual metrics
- Auto-refresh on test runs
- Export coverage reports

**Phase 4: CI/CD Integration** (4-6 hours)
- Automated test runs on push
- Block merges if tests fail
- Coverage trend tracking
- Slack/email notifications

**Phase 5: Documentation & Maintenance** (4-6 hours)
- Testing guidelines
- How to write tests
- CI/CD documentation
- Maintenance procedures

### Out of Scope

- Performance testing (separate effort)
- Security testing (separate effort)
- End-to-end testing with real browser (Playwright/Cypress)
- Load testing (separate effort)

---

## Phase 1: Backend Testing Infrastructure

**Priority:** ğŸ”´ CRITICAL
**Effort:** 8-10 hours
**Impact:** Enable all backend testing

### 1.1 Create Test Directory Structure

**Create:** `backend/tests/` directory

```
backend/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # Pytest configuration
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ supabase_fixtures.py       # Mock Supabase client
â”‚   â”œâ”€â”€ user_fixtures.py           # Test users
â”‚   â””â”€â”€ data_fixtures.py           # Test data
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_exceptions.py         # Test exception hierarchy
â”‚   â”œâ”€â”€ test_validation.py         # Test validation utils
â”‚   â”œâ”€â”€ test_database.py           # Test database utils
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_budget_tools.py   # 11 budget tool tests
â”‚       â”œâ”€â”€ test_social_tools.py   # 10 social tool tests
â”‚       â”œâ”€â”€ test_trip_tools.py     # 12 trip tool tests
â”‚       â”œâ”€â”€ test_fuel_tools.py     # 4 fuel tool tests
â”‚       â”œâ”€â”€ test_maintenance_tools.py  # 5 maintenance tests
â”‚       â”œâ”€â”€ test_transition_tools.py   # 13 transition tests
â”‚       â”œâ”€â”€ test_profile_tools.py  # 6 profile tool tests
â”‚       â”œâ”€â”€ test_shop_tools.py     # 4 shop tool tests
â”‚       â”œâ”€â”€ test_meals_tools.py    # 7 meals tool tests
â”‚       â”œâ”€â”€ test_calendar_tools.py # 3 calendar tests
â”‚       â”œâ”€â”€ test_admin_tools.py    # 2 admin tool tests
â”‚       â””â”€â”€ test_standalone_tools.py  # 11 standalone tests
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_tool_execution.py     # Tool execution flow
â”‚   â”œâ”€â”€ test_websocket.py          # WebSocket integration
â”‚   â””â”€â”€ test_api_endpoints.py      # API endpoint tests
â””â”€â”€ coverage/
    â””â”€â”€ .gitkeep
```

### 1.2 Configure Pytest

**Create:** `backend/pytest.ini`

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    --cov=app
    --cov-report=html
    --cov-report=json
    --cov-report=term-missing
    --cov-fail-under=85
    --asyncio-mode=auto
    -v
asyncio_mode = auto
```

**Create:** `backend/.coveragerc`

```ini
[run]
source = app
omit =
    */tests/*
    */migrations/*
    */__pycache__/*
    */venv/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
```

### 1.3 Create Fixtures

**Create:** `backend/tests/conftest.py`

```python
"""
Pytest Configuration and Shared Fixtures
"""
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from typing import Dict, Any

@pytest.fixture
def mock_supabase():
    """Mock Supabase client for all tests"""
    mock = MagicMock()

    # Mock successful responses
    mock.table.return_value.select.return_value.eq.return_value.single.return_value.execute.return_value = MagicMock(
        data={"id": "test-uuid", "user_id": "test-uuid"}
    )

    mock.table.return_value.insert.return_value.execute.return_value = MagicMock(
        data=[{"id": "test-uuid", "created_at": "2026-01-29T00:00:00Z"}]
    )

    mock.table.return_value.update.return_value.eq.return_value.execute.return_value = MagicMock(
        data=[{"id": "test-uuid", "updated_at": "2026-01-29T00:00:00Z"}]
    )

    mock.table.return_value.delete.return_value.eq.return_value.execute.return_value = MagicMock(
        data=[]
    )

    return mock

@pytest.fixture
def test_user_id():
    """Test user UUID"""
    return "00000000-0000-0000-0000-000000000001"

@pytest.fixture
def test_vehicle_id():
    """Test vehicle UUID"""
    return "00000000-0000-0000-0000-000000000002"

@pytest.fixture
def mock_get_supabase_client(mock_supabase):
    """Patch get_supabase_client to return mock"""
    with patch('app.core.database.get_supabase_client', return_value=mock_supabase):
        yield mock_supabase

@pytest.fixture
def mock_ai_client():
    """Mock AI client for tool tests"""
    mock = AsyncMock()
    mock.generate_text.return_value = "Mocked AI response"
    return mock
```

### 1.4 Write PAM Tool Tests (88 tools)

**Template for each tool test:**

```python
# backend/tests/unit/tools/test_fuel_tools.py
import pytest
from app.services.pam.tools.fuel.fuel_crud import (
    add_fuel_entry,
    update_fuel_entry,
    delete_fuel_entry,
    get_fuel_stats
)
from app.services.pam.tools.exceptions import ValidationError, DatabaseError

class TestAddFuelEntry:
    """Tests for add_fuel_entry tool"""

    @pytest.mark.asyncio
    async def test_add_fuel_entry_success(self, mock_get_supabase_client, test_user_id):
        """Test successful fuel entry creation"""
        result = await add_fuel_entry(
            user_id=test_user_id,
            odometer=50000,
            volume=45.5,
            price=1.50
        )

        assert result["success"] == True
        assert result["entry"]["total"] == 68.25  # 45.5 * 1.50
        assert "consumption" in result or "message" in result

    @pytest.mark.asyncio
    async def test_add_fuel_entry_invalid_user_id(self):
        """Test ValidationError for invalid UUID"""
        with pytest.raises(ValidationError) as exc_info:
            await add_fuel_entry(
                user_id="not-a-uuid",
                odometer=50000,
                volume=45.5
            )

        assert "user_id" in str(exc_info.value)
        assert "UUID" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_add_fuel_entry_negative_odometer(self, test_user_id):
        """Test ValidationError for negative odometer"""
        with pytest.raises(ValidationError) as exc_info:
            await add_fuel_entry(
                user_id=test_user_id,
                odometer=-100,
                volume=45.5,
                price=1.50
            )

        assert "odometer" in str(exc_info.value)
        assert "positive" in str(exc_info.value).lower()

    @pytest.mark.asyncio
    async def test_add_fuel_entry_insufficient_data(self, test_user_id):
        """Test ValidationError when less than 2 of 3 values provided"""
        with pytest.raises(ValidationError) as exc_info:
            await add_fuel_entry(
                user_id=test_user_id,
                odometer=50000,
                volume=45.5
                # Missing both price and total
            )

        assert "at least 2" in str(exc_info.value).lower()

    @pytest.mark.asyncio
    async def test_add_fuel_entry_smart_calculation_volume(self, mock_get_supabase_client, test_user_id):
        """Test smart calculation when volume is missing"""
        result = await add_fuel_entry(
            user_id=test_user_id,
            odometer=50000,
            price=1.50,
            total=68.25
        )

        assert result["calculated"]["volume"] == 45.5  # 68.25 / 1.50

    @pytest.mark.asyncio
    async def test_add_fuel_entry_database_error(self, test_user_id):
        """Test DatabaseError handling"""
        with patch('app.services.pam.tools.utils.database.safe_db_insert', side_effect=DatabaseError("DB error")):
            with pytest.raises(DatabaseError):
                await add_fuel_entry(
                    user_id=test_user_id,
                    odometer=50000,
                    volume=45.5,
                    price=1.50
                )
```

**Repeat for all 88 tools** with:
- âœ… Success case test
- âœ… Invalid input tests (ValidationError)
- âœ… Database error tests (DatabaseError)
- âœ… Edge case tests (boundaries, empty data)
- âœ… Business logic tests (calculations, validations)

### 1.5 Test Coverage Goals

**Per Tool Category:**

| Category | Tools | Test Files | Min Tests per Tool | Total Tests |
|----------|-------|------------|-------------------|-------------|
| Budget | 11 | 1 | 5 | 55 |
| Social | 10 | 1 | 5 | 50 |
| Trip | 12 | 1 | 5 | 60 |
| Fuel | 4 | 1 | 6 | 24 |
| Maintenance | 5 | 1 | 5 | 25 |
| Transition | 13 | 1 | 5 | 65 |
| Profile | 6 | 1 | 5 | 30 |
| Shop | 4 | 1 | 5 | 20 |
| Meals | 7 | 1 | 5 | 35 |
| Calendar | 3 | 1 | 5 | 15 |
| Admin | 2 | 1 | 5 | 10 |
| Standalone | 11 | 1 | 5 | 55 |
| **Total** | **88** | **12** | **~5** | **~440** |

**Utility Tests:**
- Exceptions: 15 tests
- Validation: 20 tests
- Database: 25 tests
- **Total utility tests:** 60

**API Endpoint Tests:**
- Health: 5 tests
- Auth: 10 tests
- PAM: 15 tests
- News: 10 tests
- Other endpoints: 40 tests
- **Total API tests:** 80

**Grand Total Backend:** ~580 tests

### 1.6 Run Tests and Generate Coverage

**Commands:**

```bash
# Run all tests
cd backend
pytest

# Run with coverage
pytest --cov=app --cov-report=html

# Run specific test file
pytest tests/unit/tools/test_fuel_tools.py -v

# Run and fail if coverage < 85%
pytest --cov-fail-under=85
```

**Acceptance Criteria:**
- [ ] All 88 PAM tools have tests
- [ ] All tests pass
- [ ] Coverage â‰¥ 85%
- [ ] HTML coverage report generated
- [ ] JSON coverage data available for dashboard

---

## Phase 2: Frontend Testing Expansion

**Priority:** ğŸŸ¡ HIGH
**Effort:** 10-12 hours
**Impact:** Comprehensive frontend coverage

### 2.1 Missing Component Tests

**PAM Voice Components** (High Priority)

```typescript
// src/__tests__/components/pam/PAMVoiceHybrid.test.tsx
describe('PAMVoiceHybrid', () => {
  it('establishes websocket connection on mount', async () => {
    // Test WebSocket initialization
  });

  it('sends message when user speaks', async () => {
    // Test voice input handling
  });

  it('displays AI response in chat', async () => {
    // Test response rendering
  });

  it('handles connection errors gracefully', async () => {
    // Test error handling
  });

  it('reconnects on connection loss', async () => {
    // Test reconnection logic
  });
});
```

**News Components** (Medium Priority)

```typescript
// src/__tests__/components/news/NewsCollapsible.test.tsx
describe('NewsCollapsible', () => {
  it('renders news items correctly', () => {
    // Test rendering
  });

  it('shows loading state while fetching', () => {
    // Test loading UI
  });

  it('handles empty news list', () => {
    // Test empty state
  });

  it('retries fetch on error', () => {
    // Test retry logic
  });
});

// src/__tests__/components/news/useNewsData.test.ts
describe('useNewsData', () => {
  it('fetches news from backend API', async () => {
    // Test API call
  });

  it('handles API errors', async () => {
    // Test error handling
  });

  it('sorts news by date', () => {
    // Test sorting
  });
});
```

**Social Components** (Medium Priority)

```typescript
// src/__tests__/components/social/*.test.tsx
// - Posts creation and display
// - Comments and likes
// - User following
// - Feed loading
```

**Dashboard Components** (Medium Priority)

```typescript
// src/__tests__/components/DashboardCards.test.tsx
// - Card rendering
// - Data fetching
// - Loading states
// - Error handling
```

**Trip Planning Components** (Medium Priority)

```typescript
// src/__tests__/components/wheels/trip-planner/*.test.tsx
// - Route planning
// - RV park search
// - Cost calculations
// - Map integration
```

**Budget Components** (Medium Priority)

```typescript
// src/__tests__/components/wins/expenses/*.test.tsx
// - Expense creation
// - Budget tracking
// - Spending analysis
// - Income tracking
```

### 2.2 Test Coverage Goals

**Frontend Test Distribution:**

| Category | Current | Target | New Tests | Effort |
|----------|---------|--------|-----------|--------|
| Auth | 12 tests | 15 tests | +3 | 30min |
| PAM Voice | 3 tests | 25 tests | +22 | 3h |
| News | 0 tests | 15 tests | +15 | 2h |
| Social | 0 tests | 30 tests | +30 | 3h |
| Dashboard | 0 tests | 20 tests | +20 | 2h |
| Trip Planning | 17 tests | 40 tests | +23 | 2.5h |
| Budget | 14 tests | 35 tests | +21 | 2.5h |
| Profile | 0 tests | 15 tests | +15 | 1.5h |
| Shop | 0 tests | 15 tests | +15 | 1.5h |
| UI Components | 17 tests | 25 tests | +8 | 1h |
| **Total** | **95** | **300+** | **~210** | **~20h** |

### 2.3 Vitest Configuration

**Update:** `vite.config.ts`

```typescript
export default defineConfig({
  plugins: [react()],
  test: {
    globals: true,
    environment: 'jsdom',
    setupFiles: './src/__tests__/setup.ts',
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html'],
      exclude: [
        'node_modules/',
        'src/__tests__/',
        '**/*.d.ts',
        '**/*.config.*',
        '**/mockData.ts',
      ],
      thresholds: {
        lines: 85,
        functions: 85,
        branches: 85,
        statements: 85
      }
    }
  }
});
```

### 2.4 Run Tests

```bash
# Run all frontend tests
npm test

# Run with coverage
npm run test:coverage

# Run specific test file
npm test PAMVoiceHybrid

# Watch mode
npm run test:watch
```

**Acceptance Criteria:**
- [ ] All missing components have tests
- [ ] Coverage â‰¥ 85% for all categories
- [ ] All tests pass
- [ ] Coverage report generated

---

## Phase 3: Testing Dashboard Integration

**Priority:** ğŸŸ¢ MEDIUM
**Effort:** 4-6 hours
**Impact:** Real-time visibility into test status

### 3.1 Backend API for Test Metrics

**Create:** `backend/app/api/testing.py`

```python
"""
Testing API - Provides real test metrics for admin dashboard
"""
import json
import subprocess
from pathlib import Path
from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse

router = APIRouter(prefix="/api/testing", tags=["testing"])

@router.get("/metrics/backend")
async def get_backend_test_metrics():
    """Run pytest and return coverage metrics"""
    try:
        # Run pytest with coverage
        result = subprocess.run(
            ["pytest", "--cov=app", "--cov-report=json", "--json-report"],
            cwd="/app/backend",
            capture_output=True,
            text=True,
            timeout=300  # 5 minutes max
        )

        # Read coverage JSON
        coverage_file = Path("/app/backend/coverage.json")
        if not coverage_file.exists():
            raise HTTPException(status_code=500, detail="Coverage data not generated")

        with open(coverage_file) as f:
            coverage_data = json.load(f)

        # Read test results
        report_file = Path("/app/backend/.report.json")
        test_data = {}
        if report_file.exists():
            with open(report_file) as f:
                test_data = json.load(f)

        return JSONResponse({
            "totalTests": test_data.get("summary", {}).get("total", 0),
            "passingTests": test_data.get("summary", {}).get("passed", 0),
            "failingTests": test_data.get("summary", {}).get("failed", 0),
            "skippedTests": test_data.get("summary", {}).get("skipped", 0),
            "coverage": {
                "lines": coverage_data["totals"]["percent_covered"],
                "branches": coverage_data["totals"]["percent_covered_display"],
                "functions": coverage_data["totals"]["percent_covered"],
                "statements": coverage_data["totals"]["percent_covered"]
            },
            "lastRun": test_data.get("created", ""),
            "duration": test_data.get("duration", 0) * 1000,  # Convert to ms
            "testFiles": [
                {
                    "name": file,
                    "coverage": data["summary"]["percent_covered"]
                }
                for file, data in coverage_data["files"].items()
            ]
        })

    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=504, detail="Tests took too long")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/metrics/frontend")
async def get_frontend_test_metrics():
    """Run Vitest and return coverage metrics"""
    try:
        # Run vitest with coverage
        result = subprocess.run(
            ["npm", "run", "test:coverage", "--", "--reporter=json"],
            cwd="/app",
            capture_output=True,
            text=True,
            timeout=300
        )

        # Parse coverage data (similar to backend)
        # Return structured metrics

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/metrics/all")
async def get_all_test_metrics():
    """Get combined backend + frontend metrics"""
    backend = await get_backend_test_metrics()
    frontend = await get_frontend_test_metrics()

    return JSONResponse({
        "backend": backend,
        "frontend": frontend,
        "combined": {
            "totalTests": backend["totalTests"] + frontend["totalTests"],
            "passingTests": backend["passingTests"] + frontend["passingTests"],
            "coverage": {
                "lines": (backend["coverage"]["lines"] + frontend["coverage"]["lines"]) / 2
            }
        }
    })

@router.post("/run")
async def run_tests(test_type: str = "all"):
    """Trigger test execution"""
    # Run tests in background
    # Return job ID
    pass
```

### 3.2 Update Frontend Hook

**Update:** `src/hooks/useTestMetrics.ts`

```typescript
export function useTestMetrics() {
  const [metrics, setMetrics] = useState<TestMetrics>({...});
  const [isLoading, setIsLoading] = useState(false);

  const fetchRealMetrics = async () => {
    setIsLoading(true);
    try {
      const response = await fetch(`${API_URL}/api/testing/metrics/all`);
      const data = await response.json();

      setMetrics({
        totalTests: data.combined.totalTests,
        passingTests: data.combined.passingTests,
        failingTests: data.backend.failingTests + data.frontend.failingTests,
        coverage: data.combined.coverage,
        lastRun: new Date().toISOString(),
        duration: data.backend.duration + data.frontend.duration
      });
    } catch (error) {
      console.error('Failed to fetch real metrics:', error);
    } finally {
      setIsLoading(false);
    }
  };

  useEffect(() => {
    fetchRealMetrics();
  }, []);

  const runTests = async () => {
    setIsLoading(true);
    await fetch(`${API_URL}/api/testing/run`, { method: 'POST' });
    await fetchRealMetrics();
    setIsLoading(false);
  };

  return { metrics, isLoading, fetchRealMetrics, runTests };
}
```

**Acceptance Criteria:**
- [ ] Testing Dashboard shows real metrics
- [ ] "Run Tests" button actually runs tests
- [ ] "Refresh" button fetches latest results
- [ ] Coverage percentages accurate
- [ ] Test counts accurate

---

## Phase 4: CI/CD Integration

**Priority:** ğŸŸ¢ MEDIUM
**Effort:** 4-6 hours
**Impact:** Automated quality gates

### 4.1 GitHub Actions Workflow

**Create:** `.github/workflows/test.yml`

```yaml
name: Test Suite

on:
  push:
    branches: [main, staging]
  pull_request:
    branches: [main, staging]

jobs:
  backend-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio

      - name: Run backend tests
        run: |
          cd backend
          pytest --cov=app --cov-report=json --cov-fail-under=85

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./backend/coverage.json
          flags: backend

  frontend-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm ci

      - name: Run frontend tests
        run: npm run test:coverage

      - name: Check coverage threshold
        run: |
          COVERAGE=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)
          if (( $(echo "$COVERAGE < 85" | bc -l) )); then
            echo "Coverage $COVERAGE% is below 85% threshold"
            exit 1
          fi

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/coverage-final.json
          flags: frontend

  integration-tests:
    runs-on: ubuntu-latest
    needs: [backend-tests, frontend-tests]

    steps:
      - uses: actions/checkout@v3

      # Run integration tests
      - name: Run integration test suite
        run: npm run test:integration
```

### 4.2 Pre-commit Hooks

**Create:** `.husky/pre-push`

```bash
#!/bin/sh
. "$(dirname "$0")/_/husky.sh"

# Run tests before pushing
echo "ğŸ§ª Running tests before push..."

# Backend tests
echo "ğŸ“¦ Backend tests..."
cd backend && pytest --cov=app --cov-fail-under=85 || exit 1
cd ..

# Frontend tests
echo "ğŸ¨ Frontend tests..."
npm run test:coverage || exit 1

echo "âœ… All tests passed! Pushing..."
```

**Acceptance Criteria:**
- [ ] Tests run automatically on push
- [ ] PRs blocked if tests fail
- [ ] Coverage tracked over time
- [ ] Notifications on failures

---

## Phase 5: Documentation & Maintenance

**Priority:** ğŸŸ¢ LOW
**Effort:** 4-6 hours
**Impact:** Team enablement

### 5.1 Testing Guidelines

**Create:** `docs/TESTING_GUIDE.md`

- How to write tests
- Testing best practices
- Running tests locally
- Debugging failed tests
- Coverage expectations

### 5.2 CI/CD Documentation

**Create:** `docs/CI_CD_GUIDE.md`

- Workflow overview
- How to fix failing tests
- Coverage requirements
- Deployment process

**Acceptance Criteria:**
- [ ] Comprehensive testing guide written
- [ ] CI/CD process documented
- [ ] Examples provided
- [ ] Troubleshooting guide included

---

## Implementation Timeline

### Week 1: Backend Testing
**Days 1-2:** Phase 1.1-1.3 (Infrastructure setup)
- Create test directories
- Configure pytest
- Create fixtures

**Days 3-5:** Phase 1.4-1.6 (Write tests)
- Write 88 PAM tool tests
- Write utility tests
- Write API tests
- Achieve 85%+ coverage

### Week 2: Frontend Testing & Integration
**Days 1-3:** Phase 2 (Frontend tests)
- Write missing component tests
- Achieve 85%+ coverage

**Days 4-5:** Phase 3 (Dashboard integration)
- Create testing API
- Connect dashboard to real data

### Week 3: CI/CD & Documentation
**Days 1-2:** Phase 4 (CI/CD)
- GitHub Actions setup
- Pre-commit hooks

**Days 3:** Phase 5 (Documentation)
- Testing guide
- CI/CD documentation

**Total:** ~15-20 working days (3 weeks)

---

## Success Metrics

### Quantitative Goals

**Backend:**
- âœ… â‰¥ 85% line coverage
- âœ… â‰¥ 85% branch coverage
- âœ… â‰¥ 85% function coverage
- âœ… â‰¥ 200 tests written
- âœ… All 88 PAM tools tested
- âœ… All tests pass
- âœ… Test execution < 5 minutes

**Frontend:**
- âœ… â‰¥ 85% line coverage
- âœ… â‰¥ 85% branch coverage
- âœ… â‰¥ 85% function coverage
- âœ… â‰¥ 300 tests written
- âœ… All major components tested
- âœ… All tests pass
- âœ… Test execution < 2 minutes

**Dashboard:**
- âœ… Real metrics displayed
- âœ… Auto-refresh working
- âœ… Run tests button functional
- âœ… Coverage trends visible

**CI/CD:**
- âœ… Automated test runs on push
- âœ… PRs blocked on test failures
- âœ… Coverage tracked over time
- âœ… < 10 minute total CI/CD time

### Qualitative Goals

- âœ… Developers confident in deployments
- âœ… Bugs caught before production
- âœ… Fast feedback on code changes
- âœ… Easy to write new tests
- âœ… Clear test failure messages

---

## Risk Mitigation

### Risk 1: Tests Take Too Long
**Mitigation:**
- Run tests in parallel
- Use faster test runners
- Optimize slow tests
- Cache dependencies in CI

### Risk 2: Flaky Tests
**Mitigation:**
- Use proper mocking
- Avoid timing-dependent tests
- Retry failed tests (max 3 times)
- Document known flaky tests

### Risk 3: Low Team Adoption
**Mitigation:**
- Comprehensive documentation
- Pair programming on first tests
- Make tests easy to write
- Celebrate coverage improvements

### Risk 4: Coverage Gaming
**Mitigation:**
- Review test quality, not just quantity
- Require meaningful assertions
- Code review test PRs
- Focus on critical paths first

---

## Acceptance Criteria

### Phase 1: Backend Testing âœ…
- [ ] All 88 PAM tools have â‰¥ 5 tests each
- [ ] Utility functions fully tested
- [ ] API endpoints tested
- [ ] Coverage â‰¥ 85%
- [ ] All tests pass in < 5 minutes
- [ ] Coverage reports generated

### Phase 2: Frontend Testing âœ…
- [ ] All major components tested
- [ ] Missing categories covered
- [ ] Coverage â‰¥ 85%
- [ ] All tests pass in < 2 minutes
- [ ] Coverage reports generated

### Phase 3: Dashboard Integration âœ…
- [ ] Testing Dashboard shows real data
- [ ] Metrics update on test runs
- [ ] Run tests button functional
- [ ] Coverage visualization accurate

### Phase 4: CI/CD Integration âœ…
- [ ] GitHub Actions running tests
- [ ] PRs blocked on failures
- [ ] Coverage tracked
- [ ] Team notified on failures

### Phase 5: Documentation âœ…
- [ ] Testing guide complete
- [ ] CI/CD guide complete
- [ ] Examples provided
- [ ] Team trained

---

## Next Steps

1. **Review and approve this PRD**
2. **Allocate 3 weeks for implementation**
3. **Start with Phase 1 (Backend testing)**
4. **Weekly check-ins on progress**
5. **Celebrate when we hit 85%+ coverage!**

---

**Status:** ğŸ“‹ Ready for Implementation
**Estimated Delivery:** 3 weeks from start
**Team Impact:** High confidence, fast deployments, fewer bugs
